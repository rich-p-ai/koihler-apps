apiVersion: batch/v1
kind: Job
metadata:
  name: export-sftp-datalake-data01
  namespace: data-analytics
  labels:
    migration: data-analytics
    pvc: sftp-datalake-data01
spec:
  template:
    metadata:
      labels:
        migration: data-analytics
        pvc: sftp-datalake-data01
    spec:
      restartPolicy: Never
      serviceAccountName: useroot
      securityContext:
        runAsUser: 0
        fsGroup: 0
      containers:
      - name: export
        image: registry.redhat.io/ubi8/ubi:latest
        command:
        - /bin/bash
        - -c
        - |
          echo "Starting export of PVC: sftp-datalake-data01"
          
          # Install required tools
          dnf install -y tar gzip
          
          # Check if source directory has data
          if [ -d "/source" ] && [ "$(find /source -mindepth 1 -maxdepth 1 | wc -l)" -gt 0 ]; then
            echo "Data found in /source, creating archive..."
            cd /source
            tar -czf "/backup/sftp-datalake-data01-data.tar.gz" .
            echo "Archive created: sftp-datalake-data01-data.tar.gz"
            echo "Archive size: $(du -h /backup/sftp-datalake-data01-data.tar.gz)"
          else
            echo "No data found in /source for sftp-datalake-data01"
            touch "/backup/sftp-datalake-data01-empty.marker"
          fi
          
          echo "Export completed for PVC: sftp-datalake-data01"
        volumeMounts:
        - name: source-data
          mountPath: /source
        - name: backup-storage
          mountPath: /backup
      volumes:
      - name: source-data
        persistentVolumeClaim:
          claimName: sftp-datalake-data01
      - name: backup-storage
        persistentVolumeClaim:
          claimName: migration-backup-storage
---
